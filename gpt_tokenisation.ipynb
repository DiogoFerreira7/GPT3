{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello how are you hope you are well\\n15496, 703, 389, 345, 2911, 345, 389, 880\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Byte pair encoding algorithm for tokenisation was popularised by GPT\n",
    "\n",
    "\n",
    "# Tokenisation can be a problem for the following things\n",
    "#     Spelling words\n",
    "#     Reversing words and other sipmle string processing\n",
    "#     Worse in non english languages such as Chinese and Japanese\n",
    "#     More trouble coding in specific languages such as python\n",
    "#         For example programming in python we waste context length because spaces in python for indentation are all separate tokens\n",
    "#     Trailing whitespace problems where other tokens are generated\n",
    "\n",
    "# Increasing number of tokens is not necessarily always better otherwise the embedding table and the softmax will always grow\n",
    "# Should be appropriately dense e.g not having spaces for every single indentation in python - in new tokenisers they are represented in a single token\n",
    "# These choices are deliberate when designing the tokenisers as it densifies python and it can attend to more code before it - the coding ability does not just rely on the architecture but also the tokeniser design \n",
    "\n",
    "\"\"\"\n",
    "Hello how are you hope you are well\n",
    "15496, 703, 389, 345, 2911, 345, 389, 880\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 164)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2171"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bit pair algorithm implementation using UTF\n",
    "\n",
    "# Strings must be tokenised into a given vocabulary that can be fed into a transformer as input\n",
    "# We want to support all languages and other special characters such as emojis\n",
    "    \n",
    "# The vocabulary in unicode is not a stable representation and it already continues changing it is also very very long\n",
    "# UTF-8 is the standard into an 8 byte string 2^8\n",
    "\n",
    "# We can use the byte pairing algorithm that will allow us to use variable length - compressed training dataset\n",
    "tokens = list(\"this is simply a test of unicode encoding in python, the longer the string the better we can test our bit pair algorithm - notice the amount of 'the' used. GPT Generated Sentece for testing - The quick brown fox jumps over the lazy dog! こんにちは世界! Добро пожаловать! ¡Hola, mundo! 你好，世界! Bonjour le monde! 안녕하세요 세계! Γειά σου Κόσμε! שלום עולם! नमस्ते दुनिया! 🌍🚀💻✨ नमस्ते, मेरा नाम है GPT-4. This is a long sentence designed to test a tokenizer's capabilities. 一只敏捷的棕色狐狸跳过了懒狗! हरिओम, यहां हम विभिन्न भाषाओं और लिपियों का प्रयोग कर रहे हैं। Lorem ipsum dolor sit amet, consectetur adipiscing elit. カタカナとひらがなも使います。 Моя цель — проверить токенизацию. ¿Puedes entender este texto? 😊✨👾🎉 Python is great for scripting! எங்கள் விஞ்ஞானிகள் நியூயார்க்கில் உள்ளனர். الطقس جميل اليوم! Будем рады видеть вас снова. ここに多くの異なる文字があります。 Это предложение становится длиннее и длиннее. 我们正在测试各种字符。 Δοκιμάζουμε διαφορετικούς χαρακτήρες. הקפיצה המהירה של השועל החום מעל הכלב העצלן! Всем привет! 🌟🌐📚👩‍💻🧑‍🚀🎨 βελτιώνουμε συνεχώς το μοντέλο μας. ¿Qué tal tu día? မင်္ဂလာပါ။ हमने बहुत सारी भाषाएँ शामिल की हैं। ताजमहल भारत में है। 🚗🚀📱💡💬🌈🙌 Этот текст продолжает расти. Qu'est-ce que vous en pensez? 今日はどうですか? Aloha ʻāina! फिर मिलेंगे। 🏖️🏔️🗽🕌🏯 🚴‍♂️🏊‍♀️⛷️🏋️‍♀️🤹‍♂️\".encode(\"utf-8\"))\n",
    "\n",
    "def get_pairs(data):\n",
    "    counts = {}\n",
    "    # Here we can use zip to iterate over consecutive elements\n",
    "    for pair in zip(data, data[1:]):\n",
    "        # We get the value, by default if it does not exist it will be 0 and we will add 1\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "pairs = get_pairs(tokens)\n",
    "# We can use sorted to sort by the value by using a for loop\n",
    "# By default python will make it descending so we can use reversed\n",
    "sorted_pairs = sorted(((v, k) for k, v in pairs.items()), reverse=True)\n",
    "sorted_pairs \n",
    "\n",
    "# ord() and chr() are inverses\n",
    "# (2, (116, 104)) is the most common pair\n",
    "# This being t followed by h\n",
    "# chr(116)\n",
    "# chr(104)\n",
    "\n",
    "# Here we can use a dictionary to get the max via a key function that is .get\n",
    "# we can use the .get function taht will compare all the values in the \n",
    "# by giving it the key= function this function pairs.get will return the value that should be filtered\n",
    "top_pair = max(pairs, key=pairs.get)\n",
    "top_pair\n",
    "\n",
    "print(top_pair)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # replace all occurrences of pairs with that id with the new index token\n",
    "    newids = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # The only time we dont want to do it is when we are at the last position\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids \n",
    "\n",
    "# tokens = merge(tokens, top_pair, 256)\n",
    "\n",
    "# We have reduced the amount of tokens used by using repeat occurrences byte pair\n",
    "# len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116,\n",
       " 104,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 115,\n",
       " 105,\n",
       " 109,\n",
       " 112,\n",
       " 108,\n",
       " 121,\n",
       " 32,\n",
       " 97,\n",
       " 268,\n",
       " 101,\n",
       " 115,\n",
       " 116,\n",
       " 32,\n",
       " 111,\n",
       " 102,\n",
       " 32,\n",
       " 117,\n",
       " 110,\n",
       " 105,\n",
       " 99,\n",
       " 111,\n",
       " 100,\n",
       " 264,\n",
       " 270,\n",
       " 99,\n",
       " 111,\n",
       " 100,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 112,\n",
       " 121,\n",
       " 116,\n",
       " 104,\n",
       " 111,\n",
       " 110,\n",
       " 44,\n",
       " 268,\n",
       " 104,\n",
       " 264,\n",
       " 108,\n",
       " 111,\n",
       " 110,\n",
       " 103,\n",
       " 101,\n",
       " 114,\n",
       " 268,\n",
       " 104,\n",
       " 264,\n",
       " 115,\n",
       " 116,\n",
       " 114,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 268,\n",
       " 104,\n",
       " 264,\n",
       " 98,\n",
       " 101,\n",
       " 116,\n",
       " 116,\n",
       " 101,\n",
       " 114,\n",
       " 32,\n",
       " 119,\n",
       " 264,\n",
       " 99,\n",
       " 97,\n",
       " 110,\n",
       " 268,\n",
       " 101,\n",
       " 115,\n",
       " 116,\n",
       " 32,\n",
       " 111,\n",
       " 117,\n",
       " 114,\n",
       " 32,\n",
       " 98,\n",
       " 105,\n",
       " 116,\n",
       " 32,\n",
       " 112,\n",
       " 97,\n",
       " 105,\n",
       " 114,\n",
       " 32,\n",
       " 97,\n",
       " 108,\n",
       " 103,\n",
       " 111,\n",
       " 114,\n",
       " 105,\n",
       " 116,\n",
       " 104,\n",
       " 109,\n",
       " 32,\n",
       " 45,\n",
       " 32,\n",
       " 110,\n",
       " 111,\n",
       " 116,\n",
       " 105,\n",
       " 99,\n",
       " 264,\n",
       " 116,\n",
       " 104,\n",
       " 264,\n",
       " 97,\n",
       " 109,\n",
       " 111,\n",
       " 117,\n",
       " 110,\n",
       " 116,\n",
       " 32,\n",
       " 111,\n",
       " 102,\n",
       " 32,\n",
       " 39,\n",
       " 116,\n",
       " 104,\n",
       " 101,\n",
       " 39,\n",
       " 32,\n",
       " 117,\n",
       " 115,\n",
       " 101,\n",
       " 100,\n",
       " 269,\n",
       " 71,\n",
       " 80,\n",
       " 84,\n",
       " 32,\n",
       " 71,\n",
       " 270,\n",
       " 101,\n",
       " 114,\n",
       " 97,\n",
       " 116,\n",
       " 101,\n",
       " 100,\n",
       " 32,\n",
       " 83,\n",
       " 270,\n",
       " 116,\n",
       " 101,\n",
       " 99,\n",
       " 264,\n",
       " 102,\n",
       " 111,\n",
       " 114,\n",
       " 268,\n",
       " 101,\n",
       " 115,\n",
       " 116,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 32,\n",
       " 45,\n",
       " 32,\n",
       " 84,\n",
       " 104,\n",
       " 264,\n",
       " 113,\n",
       " 117,\n",
       " 105,\n",
       " 99,\n",
       " 107,\n",
       " 32,\n",
       " 98,\n",
       " 114,\n",
       " 111,\n",
       " 119,\n",
       " 110,\n",
       " 32,\n",
       " 102,\n",
       " 111,\n",
       " 120,\n",
       " 32,\n",
       " 106,\n",
       " 117,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 32,\n",
       " 111,\n",
       " 118,\n",
       " 101,\n",
       " 114,\n",
       " 268,\n",
       " 104,\n",
       " 264,\n",
       " 108,\n",
       " 97,\n",
       " 122,\n",
       " 121,\n",
       " 32,\n",
       " 100,\n",
       " 111,\n",
       " 103,\n",
       " 271,\n",
       " 260,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 260,\n",
       " 171,\n",
       " 260,\n",
       " 161,\n",
       " 260,\n",
       " 175,\n",
       " 228,\n",
       " 184,\n",
       " 150,\n",
       " 231,\n",
       " 149,\n",
       " 140,\n",
       " 33,\n",
       " 262,\n",
       " 148,\n",
       " 272,\n",
       " 177,\n",
       " 209,\n",
       " 128,\n",
       " 265,\n",
       " 262,\n",
       " 191,\n",
       " 272,\n",
       " 182,\n",
       " 208,\n",
       " 176,\n",
       " 208,\n",
       " 187,\n",
       " 272,\n",
       " 178,\n",
       " 208,\n",
       " 176,\n",
       " 266,\n",
       " 209,\n",
       " 140,\n",
       " 271,\n",
       " 194,\n",
       " 161,\n",
       " 72,\n",
       " 111,\n",
       " 108,\n",
       " 97,\n",
       " 44,\n",
       " 32,\n",
       " 109,\n",
       " 117,\n",
       " 110,\n",
       " 100,\n",
       " 111,\n",
       " 271,\n",
       " 228,\n",
       " 189,\n",
       " 160,\n",
       " 229,\n",
       " 165,\n",
       " 189,\n",
       " 239,\n",
       " 188,\n",
       " 140,\n",
       " 228,\n",
       " 184,\n",
       " 150,\n",
       " 231,\n",
       " 149,\n",
       " 140,\n",
       " 271,\n",
       " 66,\n",
       " 111,\n",
       " 110,\n",
       " 106,\n",
       " 111,\n",
       " 117,\n",
       " 114,\n",
       " 32,\n",
       " 108,\n",
       " 264,\n",
       " 109,\n",
       " 111,\n",
       " 110,\n",
       " 100,\n",
       " 101,\n",
       " 271,\n",
       " 236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 234,\n",
       " 179,\n",
       " 132,\n",
       " 271,\n",
       " 206,\n",
       " 147,\n",
       " 206,\n",
       " 181,\n",
       " 206,\n",
       " 185,\n",
       " 206,\n",
       " 172,\n",
       " 32,\n",
       " 207,\n",
       " 131,\n",
       " 206,\n",
       " 191,\n",
       " 207,\n",
       " 133,\n",
       " 32,\n",
       " 206,\n",
       " 154,\n",
       " 207,\n",
       " 140,\n",
       " 207,\n",
       " 131,\n",
       " 206,\n",
       " 188,\n",
       " 206,\n",
       " 181,\n",
       " 271,\n",
       " 215,\n",
       " 169,\n",
       " 215,\n",
       " 156,\n",
       " 215,\n",
       " 149,\n",
       " 215,\n",
       " 157,\n",
       " 32,\n",
       " 215,\n",
       " 162,\n",
       " 215,\n",
       " 149,\n",
       " 215,\n",
       " 156,\n",
       " 215,\n",
       " 157,\n",
       " 33,\n",
       " 257,\n",
       " 168,\n",
       " 256,\n",
       " 174,\n",
       " 256,\n",
       " 184,\n",
       " 261,\n",
       " 141,\n",
       " 256,\n",
       " 164,\n",
       " 261,\n",
       " 135,\n",
       " 257,\n",
       " 166,\n",
       " 261,\n",
       " 129,\n",
       " 256,\n",
       " 168,\n",
       " 256,\n",
       " 191,\n",
       " 256,\n",
       " 175,\n",
       " 267,\n",
       " 271,\n",
       " 258,\n",
       " 140,\n",
       " 141,\n",
       " 258,\n",
       " 154,\n",
       " 128,\n",
       " 258,\n",
       " 146,\n",
       " 187,\n",
       " 226,\n",
       " 156,\n",
       " 168,\n",
       " 257,\n",
       " 168,\n",
       " 256,\n",
       " 174,\n",
       " 256,\n",
       " 184,\n",
       " 261,\n",
       " 141,\n",
       " 256,\n",
       " 164,\n",
       " 261,\n",
       " 135,\n",
       " 44,\n",
       " 257,\n",
       " 174,\n",
       " 261,\n",
       " 135,\n",
       " 256,\n",
       " 176,\n",
       " 267,\n",
       " 257,\n",
       " 168,\n",
       " 273,\n",
       " 174,\n",
       " 257,\n",
       " 185,\n",
       " 261,\n",
       " 136,\n",
       " 32,\n",
       " 71,\n",
       " 80,\n",
       " 84,\n",
       " 45,\n",
       " 52,\n",
       " 269,\n",
       " 84,\n",
       " 104,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 97,\n",
       " 32,\n",
       " 108,\n",
       " 111,\n",
       " 110,\n",
       " 103,\n",
       " 32,\n",
       " 115,\n",
       " 270,\n",
       " 116,\n",
       " 270,\n",
       " 99,\n",
       " 264,\n",
       " 100,\n",
       " 101,\n",
       " 115,\n",
       " 105,\n",
       " 103,\n",
       " 110,\n",
       " 101,\n",
       " 100,\n",
       " 268,\n",
       " 111,\n",
       " 268,\n",
       " 101,\n",
       " 115,\n",
       " 116,\n",
       " 32,\n",
       " 97,\n",
       " 268,\n",
       " 111,\n",
       " 107,\n",
       " 270,\n",
       " 105,\n",
       " 122,\n",
       " 101,\n",
       " 114,\n",
       " 39,\n",
       " 115,\n",
       " 32,\n",
       " 99,\n",
       " 97,\n",
       " 112,\n",
       " 97,\n",
       " 98,\n",
       " 105,\n",
       " 108,\n",
       " 105,\n",
       " 116,\n",
       " 105,\n",
       " 101,\n",
       " 115,\n",
       " 269,\n",
       " 228,\n",
       " 184,\n",
       " 128,\n",
       " 229,\n",
       " 143,\n",
       " 170,\n",
       " 230,\n",
       " 149,\n",
       " 143,\n",
       " 230,\n",
       " 141,\n",
       " 183,\n",
       " 231,\n",
       " 154,\n",
       " 132,\n",
       " 230,\n",
       " 163,\n",
       " 149,\n",
       " 232,\n",
       " 137,\n",
       " 178,\n",
       " 231,\n",
       " 139,\n",
       " 144,\n",
       " 231,\n",
       " 139,\n",
       " 184,\n",
       " 232,\n",
       " 183,\n",
       " 179,\n",
       " 232,\n",
       " 191,\n",
       " 135,\n",
       " 228,\n",
       " 186,\n",
       " 134,\n",
       " 230,\n",
       " 135,\n",
       " 146,\n",
       " 231,\n",
       " 139,\n",
       " 151,\n",
       " 33,\n",
       " 257,\n",
       " 185,\n",
       " 256,\n",
       " 176,\n",
       " 256,\n",
       " 191,\n",
       " 256,\n",
       " 147,\n",
       " 256,\n",
       " 174,\n",
       " 44,\n",
       " 257,\n",
       " 175,\n",
       " 256,\n",
       " 185,\n",
       " 273,\n",
       " 130,\n",
       " 257,\n",
       " 185,\n",
       " 256,\n",
       " 174,\n",
       " 257,\n",
       " 181,\n",
       " 256,\n",
       " 191,\n",
       " 256,\n",
       " 173,\n",
       " 256,\n",
       " 191,\n",
       " 256,\n",
       " 168,\n",
       " 261,\n",
       " 141,\n",
       " 256,\n",
       " 168,\n",
       " 257,\n",
       " 173,\n",
       " 273,\n",
       " 183,\n",
       " 273,\n",
       " 147,\n",
       " 256,\n",
       " 130,\n",
       " 257,\n",
       " 148,\n",
       " 256,\n",
       " 176,\n",
       " 257,\n",
       " 178,\n",
       " 256,\n",
       " 191,\n",
       " 256,\n",
       " 170,\n",
       " 256,\n",
       " 191,\n",
       " 256,\n",
       " 175,\n",
       " 261,\n",
       " 139,\n",
       " 256,\n",
       " 130,\n",
       " 257,\n",
       " 149,\n",
       " 267,\n",
       " 257,\n",
       " 170,\n",
       " 261,\n",
       " 141,\n",
       " 256,\n",
       " 176,\n",
       " 256,\n",
       " 175,\n",
       " 261,\n",
       " 139,\n",
       " 256,\n",
       " 151,\n",
       " 257,\n",
       " 149,\n",
       " 256,\n",
       " 176,\n",
       " 257,\n",
       " 176,\n",
       " 256,\n",
       " 185,\n",
       " 261,\n",
       " 135,\n",
       " 257,\n",
       " 185,\n",
       " 261,\n",
       " 136,\n",
       " 256,\n",
       " 130,\n",
       " 261,\n",
       " 164,\n",
       " 32,\n",
       " 76,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 109,\n",
       " 32,\n",
       " 105,\n",
       " 112,\n",
       " 115,\n",
       " 117,\n",
       " 109,\n",
       " 32,\n",
       " 100,\n",
       " 111,\n",
       " 108,\n",
       " 111,\n",
       " 114,\n",
       " 32,\n",
       " 115,\n",
       " 105,\n",
       " 116,\n",
       " 32,\n",
       " 97,\n",
       " 109,\n",
       " 101,\n",
       " 116,\n",
       " 44,\n",
       " 32,\n",
       " 99,\n",
       " 111,\n",
       " 110,\n",
       " 115,\n",
       " 101,\n",
       " 99,\n",
       " 116,\n",
       " 101,\n",
       " 116,\n",
       " 117,\n",
       " 114,\n",
       " 32,\n",
       " 97,\n",
       " 100,\n",
       " 105,\n",
       " 112,\n",
       " 105,\n",
       " 115,\n",
       " 99,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 32,\n",
       " 101,\n",
       " 108,\n",
       " 105,\n",
       " 116,\n",
       " 269,\n",
       " 227,\n",
       " 130,\n",
       " 171,\n",
       " 227,\n",
       " 130,\n",
       " 191,\n",
       " 227,\n",
       " 130,\n",
       " 171,\n",
       " 227,\n",
       " 131,\n",
       " 138,\n",
       " 260,\n",
       " 168,\n",
       " 260,\n",
       " 178,\n",
       " 227,\n",
       " 130,\n",
       " 137,\n",
       " 260,\n",
       " 140,\n",
       " 260,\n",
       " 170,\n",
       " 227,\n",
       " 130,\n",
       " 130,\n",
       " 228,\n",
       " 189,\n",
       " 191,\n",
       " 260,\n",
       " 132,\n",
       " 260,\n",
       " 190,\n",
       " 260,\n",
       " 153,\n",
       " 227,\n",
       " 128,\n",
       " 130,\n",
       " 262,\n",
       " 156,\n",
       " 265,\n",
       " 209,\n",
       " 143,\n",
       " 32,\n",
       " 209,\n",
       " 134,\n",
       " 263,\n",
       " 208,\n",
       " 187,\n",
       " 209,\n",
       " 140,\n",
       " 32,\n",
       " 226,\n",
       " 128,\n",
       " 148,\n",
       " 262,\n",
       " 191,\n",
       " 209,\n",
       " 128,\n",
       " 272,\n",
       " 178,\n",
       " 263,\n",
       " 209,\n",
       " 128,\n",
       " 274,\n",
       " 266,\n",
       " 209,\n",
       " 140,\n",
       " 32,\n",
       " 266,\n",
       " 272,\n",
       " 186,\n",
       " 263,\n",
       " 208,\n",
       " 189,\n",
       " 274,\n",
       " 208,\n",
       " 183,\n",
       " 208,\n",
       " 176,\n",
       " 209,\n",
       " 134,\n",
       " 274,\n",
       " 209,\n",
       " 142,\n",
       " 269,\n",
       " 194,\n",
       " 191,\n",
       " 80,\n",
       " 117,\n",
       " 101,\n",
       " 100,\n",
       " 101,\n",
       " 115,\n",
       " 32,\n",
       " 270,\n",
       " 116,\n",
       " 270,\n",
       " 100,\n",
       " 101,\n",
       " 114,\n",
       " 32,\n",
       " 101,\n",
       " 115,\n",
       " 116,\n",
       " 264,\n",
       " 116,\n",
       " 101,\n",
       " 120,\n",
       " 116,\n",
       " 111,\n",
       " 63,\n",
       " 32,\n",
       " 258,\n",
       " 152,\n",
       " 138,\n",
       " 226,\n",
       " 156,\n",
       " 168,\n",
       " 258,\n",
       " 145,\n",
       " 190,\n",
       " 258,\n",
       " 142,\n",
       " 137,\n",
       " 32,\n",
       " 80,\n",
       " 121,\n",
       " 116,\n",
       " 104,\n",
       " 111,\n",
       " 110,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 103,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 116,\n",
       " 32,\n",
       " 102,\n",
       " 111,\n",
       " 114,\n",
       " 32,\n",
       " 115,\n",
       " 99,\n",
       " 114,\n",
       " 105,\n",
       " 112,\n",
       " 116,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 271,\n",
       " 259,\n",
       " 142,\n",
       " 259,\n",
       " 153,\n",
       " 275,\n",
       " 141,\n",
       " 259,\n",
       " 149,\n",
       " 259,\n",
       " 179,\n",
       " 275,\n",
       " 141,\n",
       " 32,\n",
       " 259,\n",
       " 181,\n",
       " 259,\n",
       " 191,\n",
       " 259,\n",
       " 158,\n",
       " 275,\n",
       " 141,\n",
       " 259,\n",
       " 158,\n",
       " 259,\n",
       " 190,\n",
       " 259,\n",
       " 169,\n",
       " 259,\n",
       " 191,\n",
       " 259,\n",
       " 149,\n",
       " 259,\n",
       " 179,\n",
       " 275,\n",
       " 141,\n",
       " 32,\n",
       " 259,\n",
       " 168,\n",
       " 259,\n",
       " 191,\n",
       " 259,\n",
       " 175,\n",
       " 275,\n",
       " 130,\n",
       " 259,\n",
       " 175,\n",
       " 259,\n",
       " 190,\n",
       " 259,\n",
       " 176,\n",
       " 275,\n",
       " 141,\n",
       " 259,\n",
       " 149,\n",
       " 275,\n",
       " 141,\n",
       " 259,\n",
       " 149,\n",
       " 259,\n",
       " 191,\n",
       " 259,\n",
       " 178,\n",
       " 275,\n",
       " 141,\n",
       " 32,\n",
       " 259,\n",
       " 137,\n",
       " 259,\n",
       " 179,\n",
       " 275,\n",
       " 141,\n",
       " 259,\n",
       " 179,\n",
       " 259,\n",
       " 169,\n",
       " 259,\n",
       " 176,\n",
       " 275,\n",
       " 141,\n",
       " 269,\n",
       " 216,\n",
       " 167,\n",
       " 217,\n",
       " 132,\n",
       " 216,\n",
       " 183,\n",
       " 217,\n",
       " 130,\n",
       " 216,\n",
       " 179,\n",
       " 32,\n",
       " 216,\n",
       " 172,\n",
       " 217,\n",
       " 133,\n",
       " 217,\n",
       " 138,\n",
       " 217,\n",
       " 132,\n",
       " 32,\n",
       " 216,\n",
       " 167,\n",
       " 217,\n",
       " 132,\n",
       " 217,\n",
       " 138,\n",
       " 217,\n",
       " 136,\n",
       " 217,\n",
       " 133,\n",
       " 33,\n",
       " 262,\n",
       " 145,\n",
       " 209,\n",
       " 131,\n",
       " 208,\n",
       " 180,\n",
       " 263,\n",
       " 208,\n",
       " 188,\n",
       " 32,\n",
       " 209,\n",
       " 128,\n",
       " 208,\n",
       " 176,\n",
       " 208,\n",
       " 180,\n",
       " 209,\n",
       " 139,\n",
       " 262,\n",
       " 178,\n",
       " 274,\n",
       " 208,\n",
       " 180,\n",
       " 263,\n",
       " 266,\n",
       " 209,\n",
       " 140,\n",
       " 262,\n",
       " 178,\n",
       " 208,\n",
       " 176,\n",
       " 209,\n",
       " 129,\n",
       " 32,\n",
       " 209,\n",
       " 129,\n",
       " 208,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This byte pair algorithm has a hyper parameter for tokenisation where we can actually dictate how many steps we want to run it for\n",
    "#     The longer we run it for the larger the vocabulary but the shorter our encoding is so there are always tradeoffs\n",
    "#     GPT4 uses 100k tokens\n",
    "\n",
    "# Now that we have defined the code we can include it in a loop\n",
    "\n",
    "vocabulary_size = 276\n",
    "num_merges = vocabulary_size - 256\n",
    "\n",
    "# We copy the original list here so that we don't destroy it\n",
    "ids = list(tokens)\n",
    "\n",
    "# Keep track of our merges\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    # Can optimise this function later by making it so that it will get the top one and replace all the ones that match based on the same number of pair values instead of just one\n",
    "    pairs = get_pairs(ids)\n",
    "    pair = max(pairs, key = pairs.get)\n",
    "    # We can choose a new token that is 256 values higher than the current one - since we are using utf 8 it will be 256 + any value will always be out of the range\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "# Here idx is the new token that we will be replacing the pair with\n",
    "# ids is the list of the values that is being replaced\n",
    "# pair is the max pair that needs to be replaced\n",
    "\n",
    "# from 2000 to 1700 which is great\n",
    "# The greater the vocabulary elements the greater the compression ratio\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser has its own training set and preprocessing stage, we use byte pair to train the vocabulary\n",
    "# Once we haev the vocabulary and the merges we can do encoding and decoding\n",
    "# Tokenisation can take raw text into a token sequence and a token sequence back into raw text\n",
    "\n",
    "# Not all token sequences are valid utf-8 sequences\n",
    "\n",
    "# Encoding and Decoding steps\n",
    "vocabulary = {idx: bytes([idx]) for idx in range(256)}\n",
    "\n",
    "# For all the pairs of merges\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocabulary[idx] = vocabulary[p0] + vocabulary[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    #\n",
    "    tokens = b\"\".join(vocabulary[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text\n",
    "\n",
    "def encode(text):\n",
    "    # Get a list of integers from the utf encoding merge based on our trained merges vocabulary\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    # Now we have to implement the byte pair algorithm - since we are doing pairings we have to make sure that the lenght of tokens is at least 2\n",
    "    while len(tokens) >= 2:\n",
    "        pairs = get_pairs(tokens)\n",
    "        # find the key with the lowest index in the array - start with lowest before we work our way onto larger indexes\n",
    "        # Since we are using .get we can apply a fall back and if we get a pair that is not in the tokenisation language it does not occur and thus by default we set infinity\n",
    "        # Sort the pairs and return the min given the function that retrieves our token values\n",
    "        pair = min(pairs, key = lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            # We know that nothing else can be merged if everything is infinity\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        # Keep merging the pairs using the index token that we have defined in our vocabulary\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are you'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode(encode(\"Hello how are you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte Pair Encoding\n",
    "\n",
    "The algorithm in GPT-2 is not applied naively, dog will occur frequently next to other punctuation e.g dog. dog? dog! so we will be clustering semantics with punctuation this would be a suboptimal combination\n",
    "We want to enforce that some times of characters should never be merged together and enforce these merging rules on top of the byte pair algorithm\n",
    "\n",
    "GPT-2 encoder.py is the tokeniser\n",
    "OpenAI created a regex pattern that allows them to enforce rules for what parts of the text will never be merged - they used regex package in python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ' is', ' a', ' test', ' to', ' find', ' how', ' this', ' regex', '234', ' work', '346', 's']\n"
     ]
    }
   ],
   "source": [
    "import regex as re \n",
    "\n",
    "# Raw string - has things like optional strings \n",
    "#  ?\\p{L} - a letter from any language one or more times that is following an optional space\n",
    "#  ?\\p{N} - any number so even if we have Helllo345 is split to prevent the tokenisation matching\n",
    "\n",
    "# re.IGNORECASE would be useful here to make these matches work for upper and lower case versions too HOW'S doesn't work but how's works\n",
    "# The thing about this pattern is that it is very language specific too so we don't guarantee that this works for other languages\n",
    "# The last part is matching white space up to the last white space character - \"       are\", \"           you\" the extra spaces will be taken except the last one so it will always have a space before words\n",
    "gpt2_tokenisation_pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2_tokenisation_pattern, \"This is a test to find how this regex234 work346s\"))\n",
    "# By splitting spaces and words then this means this regex pattern prevents merges such as across letters, across numbers across punctuation never happen\n",
    "# ['This', ' is', ' a', ' test', ' to', ' find', ' how', ' this', ' regex', '234', ' work', '346', 's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14326, 51309, 11727, 20, 83, 925, 3001]\n"
     ]
    }
   ],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# This is the gpt4 tokeniser\n",
    "# enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# print(enc.encode(\"Random tes2345t string!!\"))\n",
    "# tiktoken.list_encoding_names()\n",
    "\n",
    "# You can download the gpt2 encoder.json which is the vocabulary\n",
    "# vocab.bpe is the merges that we have made\n",
    "# With just vocabulary and merges we can encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence piece can do both train and inference BPE tokenizers \n",
    "# used in both llama and mistral series models\n",
    "# Looks at codepoints are available and starts merging - codepoints will either get mapped to a special unknown token / encode a rare byte_fallback to bytes for rare cases\n",
    "\n",
    "# When it finds token that it has not seen it it will use byte fallback and if we end up encoding values that are unknown we will get an <unk> token that will delete those values\n",
    "# spaces end up being defined by _ characters \n",
    "# Some tokenisers will end up adding a dummy prefix to the start of sentences so that they are treated the same because \"world\" and \" world\" in \"hello world\" will be treated differently unfortunately                            \n",
    "tokens = list(\"this is simply a test of unicode encoding in python, the longer the string the better we can test our bit pair algorithm - notice the amount of 'the' used. GPT Generated Sentece for testing - The quick brown fox jumps over the lazy dog! こんにちは世界! Добро пожаловать! ¡Hola, mundo! 你好，世界! Bonjour le monde! 안녕하세요 세계! Γειά σου Κόσμε! שלום עולם! नमस्ते दुनिया! 🌍🚀💻✨ नमस्ते, मेरा नाम है GPT-4. This is a long sentence designed to test a tokenizer's capabilities. 一只敏捷的棕色狐狸跳过了懒狗! हरिओम, यहां हम विभिन्न भाषाओं और लिपियों का प्रयोग कर रहे हैं। Lorem ipsum dolor sit amet, consectetur adipiscing elit. カタカナとひらがなも使います。 Моя цель — проверить токенизацию. ¿Puedes entender este texto? 😊✨👾🎉 Python is great for scripting! எங்கள் விஞ்ஞானிகள் நியூயார்க்கில் உள்ளனர். الطقس جميل اليوم! Будем рады видеть вас снова. ここに多くの異なる文字があります。 Это предложение становится длиннее и длиннее. 我们正在测试各种字符。 Δοκιμάζουμε διαφορετικούς χαρακτήρες. הקפיצה המהירה של השועל החום מעל הכלב העצלן! Всем привет! 🌟🌐📚👩‍💻🧑‍🚀🎨 βελτιώνουμε συνεχώς το μοντέλο μας. ¿Qué tal tu día? မင်္ဂလာပါ။ हमने बहुत सारी भाषाएँ शामिल की हैं। ताजमहल भारत में है। 🚗🚀📱💡💬🌈🙌 Этот текст продолжает расти. Qu'est-ce que vous en pensez? 今日はどうですか? Aloha ʻāina! फिर मिलेंगे। 🏖️🏔️🗽🕌🏯 🚴‍♂️🏊‍♀️⛷️🏋️‍♀️🤹‍♂️\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "\n",
    "# The more tokens we have the larger our vocabulary size will be and our initial token embedding table will grow as well as our last linear layer that will be predicting the next token in the sentence will be too large\n",
    "# Too large of chunks the model does not have enough time to process all the links and data as we are compressing too much information at once\n",
    "# Freezing the base model and training arbitrary parts of it for finde tuning / adding new tokens can be a good idea\n",
    "\n",
    "# If we have long prompts we can introduce new tokens and train the model by trainig teh new tokens - the behaviour should be identical but we can compress this new prompt to get identical performance called distillation\n",
    "\n",
    "# Different modality transformers need to have different tokenisations\n",
    "# We can take an image and truncate it into integers and become tokens\n",
    "# Visual patches that can be used ans a representation for models with visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling words can be difficult because characters are broken into tokens that are fairly long\n",
    "# If we have a single token like .DefaultCellStyle then if we ask the model how many \"l\" are in it, the model will return the wrong answer\n",
    "\n",
    "# Other basic mapings such as reversing a string are quite difficult due to tokenisation\n",
    "# However if you ask it to break up the string with spaces adn then reverse it it can do it perfectly\n",
    "\n",
    "# Translation can also be quite difficult for models - as we get different token sizes e.g Hello to korean will blow up into 3 tokens \n",
    "# Tokenisation can also affect addition as it will have different combinations of characters that are all different sizes e.g 1/324 or 32/52 342/1 separate tokens\n",
    "\n",
    "# A lot of tokenisation problems with spaces in python or programming is terible since it will dramatically reduce the context length that the model can attend to - a lot is wasted\n",
    "\n",
    "# We are searching the space of tokens where if retokenised they would be of high probability\n",
    "\n",
    "# Special tokens such as SolidGoldMagikarp - these tokens are from reddit user names\n",
    "# The tokenisation dataset was very different from the training dataset - potentially this user may have been a common person that would post a lot this string would occur many times and thus get merged to a single individual token in a vocabulary of 50,000\n",
    "# This tokenisation dataset has those strings but when you train the model this data was not present and thus it would never be trained as the training and tokenisation data was different\n",
    "# This token would never be trained - if evoked at run time then it would give undefined behaviour and these weird tokens that have never been trained would be out of distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
