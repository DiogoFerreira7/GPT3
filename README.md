Note: Check if there are any nice tips for read me / markdown to format it nicely

## LLM Journey

This project follows the principle of iteratively implementing small building blocks each coming closer to the original GPT3 (124M) implementation.

#### The following was the order of the implementations:
- Bigram model
- MLP
- Transformer
- Tokeniser
- GPT3

### Papers

The following papers were read and used to match the GPT3 implementation to its true origin, understand the separate components and optimise the model

Attention is all you need

Flash attention & flash attention 2

gpt 3 / 2

cuda paper explaining bfloat16

### Credits

open ai implementation
pytorch
hugging face repositories and implementation
andrej Karpathy
cuda resources paper - link the one for bfloat16

put other resources here

