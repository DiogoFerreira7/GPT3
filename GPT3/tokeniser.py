# TODO figure out how to train the tokeniser
    # Should probably find a better dataset - the larger thee vocabulary the better
    # TODO separate the dataloader from the other class as and use it to train the tokeniser

# TODO Create tokeniser class that can be imported and can load and save the values trained for it
    # This can be good practice for loading and storing values
    # If need be do the loading and storing for the model first and then tokeniser

class Tokeniser:
    # TODO rename everything and organise it all to make sense, comment everything too

    def __init__(self):
        self.tokens = list("this is simply a test of unicode encoding in python, the longer the string the better we can test our bit pair algorithm - notice the amount of 'the' used. GPT Generated Sentece for testing - The quick brown fox jumps over the lazy dog! ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ! Ğ”Ğ¾Ğ±Ñ€Ğ¾ Ğ¿Ğ¾Ğ¶Ğ°Ğ»Ğ¾Ğ²Ğ°Ñ‚ÑŒ! Â¡Hola, mundo! ä½ å¥½ï¼Œä¸–ç•Œ! Bonjour le monde! ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„! Î“ÎµÎ¹Î¬ ÏƒÎ¿Ï… ÎšÏŒÏƒÎ¼Îµ! ×©×œ×•× ×¢×•×œ×! à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾! ğŸŒğŸš€ğŸ’»âœ¨ à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤¹à¥ˆ GPT-4. This is a long sentence designed to test a tokenizer's capabilities. ä¸€åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’ç‹—! à¤¹à¤°à¤¿à¤“à¤®, à¤¯à¤¹à¤¾à¤‚ à¤¹à¤® à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤”à¤° à¤²à¤¿à¤ªà¤¿à¤¯à¥‹à¤‚ à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚à¥¤ Lorem ipsum dolor sit amet, consectetur adipiscing elit. ã‚«ã‚¿ã‚«ãƒŠã¨ã²ã‚‰ãŒãªã‚‚ä½¿ã„ã¾ã™ã€‚ ĞœĞ¾Ñ Ñ†ĞµĞ»ÑŒ â€” Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Â¿Puedes entender este texto? ğŸ˜Šâœ¨ğŸ‘¾ğŸ‰ Python is great for scripting! à®à®™à¯à®•à®³à¯ à®µà®¿à®à¯à®à®¾à®©à®¿à®•à®³à¯ à®¨à®¿à®¯à¯‚à®¯à®¾à®°à¯à®•à¯à®•à®¿à®²à¯ à®‰à®³à¯à®³à®©à®°à¯. Ø§Ù„Ø·Ù‚Ø³ Ø¬Ù…ÙŠÙ„ Ø§Ù„ÙŠÙˆÙ…! Ğ‘ÑƒĞ´ĞµĞ¼ Ñ€Ğ°Ğ´Ñ‹ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ²Ğ°Ñ ÑĞ½Ğ¾Ğ²Ğ°. ã“ã“ã«å¤šãã®ç•°ãªã‚‹æ–‡å­—ãŒã‚ã‚Šã¾ã™ã€‚ Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ. æˆ‘ä»¬æ­£åœ¨æµ‹è¯•å„ç§å­—ç¬¦ã€‚ Î”Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¿ÏÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚. ×”×§×¤×™×¦×” ×”××”×™×¨×” ×©×œ ×”×©×•×¢×œ ×”×—×•× ××¢×œ ×”×›×œ×‘ ×”×¢×¦×œ×Ÿ! Ğ’ÑĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚! ğŸŒŸğŸŒğŸ“šğŸ‘©â€ğŸ’»ğŸ§‘â€ğŸš€ğŸ¨ Î²ÎµÎ»Ï„Î¹ÏÎ½Î¿Ï…Î¼Îµ ÏƒÏ…Î½ÎµÏ‡ÏÏ‚ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¼Î±Ï‚. Â¿QuÃ© tal tu dÃ­a? á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«á‹ à¤¹à¤®à¤¨à¥‡ à¤¬à¤¹à¥à¤¤ à¤¸à¤¾à¤°à¥€ à¤­à¤¾à¤·à¤¾à¤à¤ à¤¶à¤¾à¤®à¤¿à¤² à¤•à¥€ à¤¹à¥ˆà¤‚à¥¤ à¤¤à¤¾à¤œà¤®à¤¹à¤² à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤¹à¥ˆà¥¤ ğŸš—ğŸš€ğŸ“±ğŸ’¡ğŸ’¬ğŸŒˆğŸ™Œ Ğ­Ñ‚Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑ‚Ğ¸. Qu'est-ce que vous en pensez? ä»Šæ—¥ã¯ã©ã†ã§ã™ã‹? Aloha Ê»Äina! à¤«à¤¿à¤° à¤®à¤¿à¤²à¥‡à¤‚à¤—à¥‡à¥¤ ğŸ–ï¸ğŸ”ï¸ğŸ—½ğŸ•ŒğŸ¯ ğŸš´â€â™‚ï¸ğŸŠâ€â™€ï¸â›·ï¸ğŸ‹ï¸â€â™€ï¸ğŸ¤¹â€â™‚ï¸".encode("utf-8"))


    def byte_pair_algorithm(self):
        # This byte pair algorithm has a hyper parameter for tokenisation where we can actually dictate how many steps we want to run it for
        # The longer we run it for the larger the vocabulary but the shorter our encoding is so there are always tradeoffs
        # GPT4 uses 100k tokens

        # Hyperparameters
        vocabulary_size = 276
        # TODO figure out why this 256 is here 
        num_merges = vocabulary_size - 256

        # We copy the original list here so that we don't destroy it
        ids = list(self.tokens)

        # Keep track of our merges
        merges = {}
        for i in range(num_merges):
            # Can optimise this function later by making it so that it will get the top one and replace all the ones that match based on the same number of pair values instead of just one
            pairs = self.get_pairs(ids)
            pair = max(pairs, key = pairs.get)
            # We can choose a new token that is 256 values higher than the current one - since we are using utf 8 it will be 256 + any value will always be out of the range
            idx = 256 + i
            ids = self.merge(ids, pair, idx)
            merges[pair] = idx

        return ids
        
    def get_sorted_pairs(self, data):
        counts = {}
        # Here we can use zip to iterate over consecutive elements
        for pair in zip(data, data[1:]):
            # We get the value, by default if it does not exist it will be 0 and we will add 1
            counts[pair] = counts.get(pair, 0) + 1

        return sorted(((v, k) for k, v in counts.items()), reverse=True)
    
    def get_top_pair(self, pairs):
        return max(pairs, key=pairs.get)
    
    def merge(self, ids, pair, idx):
        # replace all occurrences of pairs with that id with the new index token
        newids = []

        i = 0
        while i < len(ids):
            # The only time we dont want to do it is when we are at the last position
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
                newids.append(idx)
                i += 2
            else:
                newids.append(ids[i])
                i += 1
        return newids 
    
