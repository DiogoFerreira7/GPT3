
Evaluating
c:\Users\diogo\Desktop\VS\ML\NNZeroToHero\GPT\GPT3\gpt.py:262: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
Validation Loss: 3.25680
Sampling
I am a doctor, let me teach you about that. And you'll never understand me not knowing what I'm doing. And you won't know that I'm the man who is responsible for the problems that plague the world
I am a doctor, let me teach you about a fact."
"Very well."
"Well then, do you like to die at night because I am a doctor?"
"Well you do not
Traceback (most recent call last):
  File "c:\Users\diogo\Desktop\VS\ML\NNZeroToHero\GPT\GPT3\main.py", line 123, in <module>
    torch_compile=False,
    ^^^^^^^^^^^^^^^
  File "c:\Users\diogo\Desktop\VS\ML\NNZeroToHero\GPT\GPT3\trainer.py", line 145, in start
    "tokens_per_second": tokens_per_second,
                         ^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'tokens_per_second' where it is not associated with a value