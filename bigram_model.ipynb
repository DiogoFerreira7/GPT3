{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# Here we test for whether a GPU is available since it will make processing much faster\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "\n",
      "Tokenisation\n",
      "[32, 43, 57, 58, 47, 52, 45, 1, 43, 52, 41, 53, 42, 47, 52, 45]\n",
      "Testing encoding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport tiktoken \\nenc = tiktoken.get_encoding(\"gpt2\")\\nenc.n_vocab\\nenc.encode(\"testing encoding\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Defining the possible set of characters that the model can see\n",
    "# Character level tokenisers give us very long sequences but very small codebooks \n",
    "# and simple decode and encode functions\n",
    "characters = sorted(list(set(text)))\n",
    "vocabulary_size = len(characters)\n",
    "print(\"\".join(characters))\n",
    "print(vocabulary_size)\n",
    "\n",
    "print(\"\\nTokenisation\")\n",
    "\n",
    "# Tokenise means converting the raw text into a sequence of integers\n",
    "# There are many other ways to define schemas for tokenisation\n",
    "stoi = {char:i for i, char in enumerate(characters)}\n",
    "itos = {i:char for i, char in enumerate(characters)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[value] for value in l])\n",
    "\n",
    "print(encode(\"Testing encoding\"))\n",
    "print(decode(encode(\"Testing encoding\")))\n",
    "# Sentence piece is what google uses, a subword tokeniser, we are not encoding entire words or individual characters\n",
    "# Tiktoken is what gpt uses and has very good performance, much faster than the hugging face tokeniser\n",
    "\n",
    "\"\"\"\n",
    "import tiktoken \n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.n_vocab\n",
    "enc.encode(\"testing encoding\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56,  ..., 43, 56, 43])\n",
      "tensor([12,  0,  0,  ..., 45,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "# Creating and splitting our dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "\n",
    "# Now we can split our dataset\n",
    "n = int(0.9 * len(data))\n",
    "train_dataset = data[:n]\n",
    "validation_dataset = data[n:]\n",
    "\n",
    "print(train_dataset)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataloader\n",
    "# Block size / context length of training our data\n",
    "# We want the transformer that we train to be used to seeing a number of characters from 1 to block size this is especially useful for inference\n",
    "\n",
    "# Batch size is how many independent sequences we see in the forward / backward pass of our transformer\n",
    "# Batch size just dictates how many independent sequences we can process in parallel\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # Choose the dataset that we will be splitting into batches\n",
    "    data = train_dataset if split == \"train\" else validation_dataset\n",
    "\n",
    "    # Make sure that we limit from the length - block size since that is the lasdt index that can ge that sequence length\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    # the target dataset is just shifted up by 1\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    # If we are testing for GPUs we have to make sure that our dataloader takes it to the device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y \n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    for block in range(block_size):\n",
    "        context = xb[batch, :block_size+1]\n",
    "        target = yb[batch, block]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "4.5199713706970215\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Using nn.Embedding we can create a very simple bigram language model that creates an embedding table for counting\n",
    "        # Creates a tensor of shape vocabulary_size x vocabulary_size\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "\n",
    "    def forward(self, index, targets = None):\n",
    "        # Batch, Time, Channel (batch, block, channel) - B, T, C is the scores for our sequence based on the identity of an individual token\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # We have the B, T, C tensor - pytorch wants a B, C, T tensor instead as an input so we have to reshape our logits\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # Negative log likelikehod is implemented and called the cross entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss \n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(index)\n",
    "            # Get the last element in our time dimension as this is the element that we have to predict and that comes next - this wil leventaully be appended to our time context\n",
    "            logits = logits[:, -1, :]\n",
    "            # after the get the logits of our predictions we can softmax them\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            # We now concatenate our values and now we have the batch but with teh T values with their new concatenated values\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "            \n",
    "    \n",
    "m = BigramLanguageModel(vocabulary_size)\n",
    "out, loss = m(xb, yb)\n",
    "\n",
    "# Because we have 65 charactes in our dataset we should expect during initialisation that our cross entropy / negative log likelihood under a uniform distribution should be \n",
    "# - ln(1/65) which is around 4.17\n",
    "print(out.shape)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use our generation that continuously appends indexes to eachotehr and decode the values that we generated\n",
    "# print(decode().tolist())\n",
    "# decode(m.generate(index = torch.zeros((1, 1), dtype = torch.long), max_new_tokens=500)[0].tolist())\n",
    "\n",
    "# We can estimate our loss whilst we are trainig our model\n",
    "\n",
    "evaluation_iterations = 100\n",
    "\n",
    "# Decorator that prevents gradient updates so we can run inference on our model\n",
    "@torch.no_grad()\n",
    "def loss_estimation():\n",
    "    # Dictionary storing the averages for both split kinds\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    # Going through both the splits, then saving the number of evaluation iterations on a prezeroed tensor\n",
    "    # Updating the losses for every evaluation compared to each random batch\n",
    "    # Then we get the mean\n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        losses = torch.zeros(evaluation_iterations)\n",
    "        for iter in range(evaluation_iterations):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[iter] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4574451446533203\n"
     ]
    }
   ],
   "source": [
    "# AdamW works really well, typical good setting for lr is 3e-4 however with smaller networks we can get away with much higher learning rates\n",
    "optimiser = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "evaluation_interval = 500\n",
    "max_iterations = 2000\n",
    "for step in range(max_iterations):\n",
    "\n",
    "    if step % evaluation_interval == 0:\n",
    "        losses = loss_estimation()\n",
    "\n",
    "    xb, yb = get_batch(\"valid\")\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
