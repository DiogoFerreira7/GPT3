{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "2\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(len(words))\n",
    "# Smallest being 2 characters and 15 being the largest\n",
    "print(min([len(word) for word in words]))\n",
    "print(max([len(word) for word in words]))\n",
    "words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram language models are just looking at two characters, the on we are given and predicting the next character in the sequence.\n",
    "# Bigram model predicts the local characters - bigrams and simple and weak models\n",
    "\n",
    "bigram_map = {}\n",
    "\n",
    "# Zip will take two iterators and create an iterator over two tuples of the list - if any of the iterators is larger than the other zip() will handle it by terminating it \n",
    "# so we can always use zip to package two characters\n",
    "for word in words:\n",
    "    # in a bigram we can define how it starts and ends\n",
    "    # bigrams can be done by counting how often these words appear after another\n",
    "    chs = [\"<S>\"] + list(word) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        # This will allow us to set an entry within a dictionary to increasing values with a default of 0 if our .get() does not find it within the dictionary\n",
    "        bigram_map[bigram] = bigram_map.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return the tuples of key values and the counts \n",
    "# we can sort by the key being the key value indexing the values and thus we get the most probable ones, note here we have the negative value which allows us to do it in descending order\n",
    "sorted(bigram_map.items(), key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((3, 5), dtype=torch.int32)\n",
    "# By default they are torch.float32 adn thus single precision\n",
    "a.dtype\n",
    "# Indexing using 1, 3 commas just\n",
    "a[1, 3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "characters = sorted(list(set(\"\".join(words)))) # - throw out duplicates of the lowercase characters and it should return the values correctly\n",
    "# If we simply use a set within a string or something else we will get all unique possible characters that could be within it\n",
    "stoi = {s:i+1 for i, s in enumerate(characters)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "# we can get our original mapping function .items() which wil lshow the key value paairs and simply return the inverse of them using value:second_value\n",
    "# stoi.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_map = {}\n",
    "\n",
    "# Zip will take two iterators and create an iterator over two tuples of the list - if any of the iterators is larger than the other zip() will handle it by terminating it \n",
    "# so we can always use zip to package two characters\n",
    "for word in words:\n",
    "    # in a bigram we can define how it starts and ends\n",
    "    # bigrams can be done by counting how often these words appear after another\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        prior_index = stoi[ch1]\n",
    "        post_index = stoi[ch2]\n",
    "\n",
    "        bigram = (prior_index, post_index)\n",
    "        N[bigram] += 1\n",
    "        # # This will allow us to set an entry within a dictionary to increasing values with a default of 0 if our .get() does not find it within the dictionary\n",
    "        # bigram_map[bigram] = bigram_map.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
       "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
       "            0,   83,    0],\n",
       "        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n",
       "          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n",
       "            0,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n",
       "            0,   14,    2],\n",
       "        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n",
       "           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n",
       "            0,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "            0,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n",
       "            0,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n",
       "            0,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "            0, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n",
       "            0,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n",
       "            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n",
       "            0,    0,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "            0,  215,   10],\n",
       "        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n",
       "          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n",
       "           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n",
       "            0,  121,    0],\n",
       "        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n",
       "            0,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n",
       "           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x177ca6ceb40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# However the problem with plt.imshow(array) is that our bigram mapping is currently set as characters (mapping to indexes within the array)\n",
    "# What we can do is using an inverse function of character to index we create index to character\n",
    "\n",
    "# We can use matplotlib to show us an array that is storing our bigram pairs, the highest ones will be the most probable values, e.g we can see that a follows a lot of values\n",
    "# when we index into torch tensors we have to use N[i, j].item() which will return that individual integer and we can use plt.text() to plot the characters using our mapping itos() and the value\n",
    "plt.figure(figsize=(6, 6))\n",
    "# we can change the colourmap in matplotlib\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "\n",
    "# notice how at the bottom we have <E> character not being followed anything by definition and <S> not followign anytihng either, they take up lots of space and we have lots of brackets that take up space\n",
    "# Instead of 2 special tokens then we can use one special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use floats we need to divi~e and normalise them and thus we need floats for accuracy\n",
    "p = N[0].float()\n",
    "p = p / sum(p)\n",
    "# Now we have normalised them and we can sample from this distribution - we can use a multinomial - multinomial probability distribution\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can make a deterministic torch generator object, if we apply a seed\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# number betwen 0 and 1 that are sampled now we can normalise them and use torch.multinomial which will take the probaiility distribution, replacement means that when we draw an element we can put it back into them\n",
    "# replacement is by default false within this generator\n",
    "index = torch.multinomial(p, num_samples = 1, replacement=True, generator=g)\n",
    "# Then we can get the index that we used to sample from a multinomial\n",
    "itos[index.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "ksahnaauranilevias.\n",
      "dedainrwieta.\n",
      "ssonielylarte.\n",
      "faveumerifontume.\n",
      "phynslenaruani.\n",
      "core.\n",
      "yaenon.\n",
      "ka.\n",
      "jabdinerimikimaynin.\n",
      "anaasn.\n",
      "ssorionsush.\n",
      "dgossmitan.\n",
      "il.\n",
      "le.\n",
      "pann.\n",
      "that.\n",
      "janreli.\n",
      "isa.\n",
      "dyn.\n",
      "rijelumemahaunayaleva.\n",
      "cararr.\n",
      "jenh.\n",
      "anarta.\n",
      "maly.\n",
      "abely.\n",
      "a.\n",
      "i.\n",
      "lavadoni.\n",
      "themielyawat.\n",
      "f.\n",
      "modam.\n",
      "tavilitikiesaloeverin.\n",
      "n.\n",
      "e.\n",
      "kalbrenelah.\n",
      "anen.\n",
      "ch.\n",
      "k.\n",
      "jan.\n",
      "odridrdenanialilpergha.\n",
      "tezralelia.\n"
     ]
    }
   ],
   "source": [
    "# Creating an initial bigram model prototype\n",
    "generator = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Laplacian smoothing, adding a value of 1 to everything and thus nothing is infinitely unlikely\n",
    "N = (N + 1).float()\n",
    "\n",
    "# Take the row that we are on - our bigram will have the next word probabilities already estimated through counting\n",
    "# Normalising our probability distribution\n",
    "# P.sum() will give a sum that will output the entire array, every single count, if we look at the documentation we can provide teh input array but also the dimension\n",
    "# keepdim if we pass it as false, hte dimension is squeezed and collapses dimensions\n",
    "\n",
    "# summing across axis 0 will sum the 0th dimension and give us the counts [27, 27] wil lturn to [1, 27]\n",
    "\n",
    "# [27, 27] broadcasting semantics in pytorch will allow you to divide these two [1, 27]\n",
    "# we must make sure that each tensor has at least one dimension, must either be equal or one of them is one and align them\n",
    "P = N / N.sum(1, keepdim=True)\n",
    "# When dividing by these we are stretching out this 1 dimension to match this 27 dimension, this copying / stretching is just simply applied to everything else and elementwise division is applied\n",
    "for i in range(50):\n",
    "    output = []\n",
    "    # We need this to be the index that we start with as our starting character in our case this was the <S> and is now . \n",
    "    index = 0\n",
    "    while True:\n",
    "        index = torch.multinomial(P[index], num_samples=1, replacement=True, generator=generator).item()\n",
    "        output.append(itos[index])\n",
    "        # If we get our end character (index 0) terminate\n",
    "        if index == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get the sum of the values along the rows if we sum along dimension 0 we will get\n",
    "# This dimension is unintuitive but it is done because pytorch collapses the dimension you choose, since axis 0 is the row we are collapsing the entire thing to axis 0 to be a row and thus summing all columns\n",
    "# since we want to collapse all rows to be one column which adds up the entire row we want it to collapse column wise and thus need dimension one\n",
    "# without keep dimension it will all collapse into one so we want to make sure we keep dimension so that we can broadcast it across the entire array for normalisation\n",
    "# N / N.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting efficiency\n",
    "# Creates a new tensor that is stored into N\n",
    "# N = N / N.sum(1, keepdim=True)\n",
    "\n",
    "# This will do it in place and is actually preferred\n",
    "# N /= N.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want to evaluate the quality of this model currently using a single metric\n",
    "# The probability that the model assigns to every single bigram\n",
    "\n",
    "# Maximum likelihood estimation - we can use the likelihood - the product of all the probabilities, the probability of the entire dataset given the model so the probability should be as high as possible\n",
    "# Since the product will tend to 0 we can use the log likelihood\n",
    "# Log likelihood will return a negative nmber so we use negative log likelihod we can also just sum them up\n",
    "# Negative log likelihood is a very nice loss function \n",
    "# we can also use an average of hte log likelihood per character / word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use some laplacian smoothing on our model because if we have 0 then we will get a MLE of infinity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
