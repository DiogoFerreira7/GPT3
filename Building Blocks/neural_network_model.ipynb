{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network is still a bigram character language model, outputs the probabaility distribution and make the guesses for what is likely to follow that character\n",
    "# Gradient based optimisation can tune the parameters of this network\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "characters = sorted(list(set(\"\".join(words)))) # - throw out duplicates of the lowercase characters and it should return the values correctly\n",
    "# If we simply use a set within a string or something else we will get all unique possible characters that could be within it\n",
    "stoi = {s:i+1 for i, s in enumerate(characters)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "# Training set\n",
    "for word in words[:1]:\n",
    "    # in a bigram we can define how it starts and ends\n",
    "    # bigrams can be done by counting how often these words appear after another\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        prior_index = stoi[ch1]\n",
    "        post_index = stoi[ch2]\n",
    "        # print(ch1, ch2)\n",
    "        xs.append(prior_index)\n",
    "        ys.append(post_index)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xs = torch.Tensor(xs)\\nys = torch.Tensor(ys)\\n\\nxs = torch.tensor(xs)\\nys = torch.tensor(ys)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.Tensor and torch.tensor are both possible ways to do it but we should be very careful\n",
    "# tensor will infer the data type\n",
    "# Tensor will normally just keep it to float32 and will always be float32\n",
    "\n",
    "\"\"\"xs = torch.Tensor(xs)\n",
    "ys = torch.Tensor(ys)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plugging in indexes into the neural network does not make sense but we can use onehot encoding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# It is far better to not allow it to guess the values, we are far better off specifying how many total classes there are\n",
    "x_encoding = F.one_hot(xs, num_classes=27).float()\n",
    "print(x_encoding.shape)\n",
    "\n",
    "plt.imshow(x_encoding)\n",
    "x_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After applying one hot encoding we have to be careful since this wil lreturn us integers but we need floats to be returned\n",
    "x_encoding.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n",
      "torch.Size([27, 27])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Fills a tensor with random numbers drawn from a standard normal distribution\n",
    "# the shape here being 27, 1 indicates th at we have 27 weights conneced to the inputs from 1 neuron\n",
    "# which will give an output from all 27 inputs\n",
    "# However we can use far more thatn just one neuron \n",
    "W = torch.randn((27, 27))\n",
    "# If we use the @ operator we do the matrix multiplication and we multiply them\n",
    "# print(x_encoding.shape)\n",
    "# print(W.shape)\n",
    "# Their shapes have the matching 27 values\n",
    "\n",
    "# This is the most simple neural net that does not even have a non linearity or a bias\n",
    "# we are simply just trying to produce a probability distribution here and we are essentially keeping\n",
    "# 27x27 for all possible character combinations\n",
    "# We have evaluated simultaneously the 5 inputs into the neurons and evaluated them using the input weights\n",
    "matrix_multiplication = x_encoding @ W \n",
    "\n",
    "# Now if we use 27 neurons with 27 input weights each we get a 5 x 27 since we have 5 different bigrams\n",
    "# and for each we applied the weights to 27 neurons now we have all their outputs\n",
    "\n",
    "# Exponentiated outputs allows us to interpret an equivalent of counts they are positive numbers\n",
    "# they can take on various log count values based on the weights\n",
    "# We are interpreting the weights learned to be the log of counts - they are called logits\n",
    "# log of counts is the logits, equivalent to the N matrix but with each value with log aplied\n",
    "counts = matrix_multiplication.exp()\n",
    "\n",
    "# By exponentiating what we are assumming to be the log of counts we just get teh counts and thus should be equal to N\n",
    "# Now we have an output from the neural network that was the logits that are exponentiated to get counts\n",
    "# and these counts are then noramlised meaning that we have probabilities\n",
    "\n",
    "# Taking the logits and summing them after exponentation is jsut softmax\n",
    "probabilities = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "# Because all of these are differentiable we can backpropagate and optimise the values\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During this forward path we are creating this full computational graph that will be used for our .backward()\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "# Forward pass for the model\n",
    "x_encoding = F.one_hot(xs, num_classes=27).float()\n",
    "logits = x_encoding @ W\n",
    "counts = logits.exp()\n",
    "\n",
    "# Probabilities using softmax\n",
    "probs = counts / counts.sum(1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0928, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Calculation\n",
    "# torch.arange(5) # This will allow us to index our dataset correctly by taking the ith value and then using the ys (actual) for hte loss function\n",
    "# So because we are using essentially one hot encoding and we are taking the value that has one on it\n",
    "# we take the ys value and we want to maximise that - the value that is part of the ys index will be the one we care about\n",
    "# that will affect our loss\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "# None is the lack of a gradient and thus is equal to 0\n",
    "# After we set our gradient values to 0 and we do .backward() which will calculate all the derivatives according to our value\n",
    "# we have to make sure to set requires_grad = True (for optimisation reasons)\n",
    "W.grad = None \n",
    "# Make sure we have requires_grad \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we have a positive gradient then increasing it will give us an increase in loss so we want\n",
    "# to make sure that we go in the negative direction of optimising it\n",
    "W.grad\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch tensor is now updated\n",
    "W.data += 0.2 * -W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7798, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# During this forward path we are creating this full computational graph that will be used for our .backward()\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "# Gradient Descent iterations\n",
    "for k in range(100):\n",
    "    # Forward pass for the model\n",
    "    x_encoding = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = x_encoding @ W\n",
    "    counts = logits.exp()\n",
    "\n",
    "    # Probabilities using softmax\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "    # Loss function\n",
    "    loss = -probs[torch.arange(5), ys].log().mean()\n",
    "\n",
    "    W.grad = None\n",
    "    # Make sure we have requires_grad \n",
    "    loss.backward()\n",
    "    W.data += 0.05 * -W.grad\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0311, 0.0430, 0.0018, 0.0065, 0.0664, 0.0722, 0.0310, 0.0696, 0.0269,\n",
       "         0.0404, 0.0023, 0.0630, 0.0212, 0.0073, 0.0062, 0.0168, 0.0827, 0.0176,\n",
       "         0.0430, 0.0209, 0.0198, 0.0256, 0.0543, 0.0191, 0.1379, 0.0070, 0.0661],\n",
       "        [0.0205, 0.0572, 0.0120, 0.1230, 0.0421, 0.0592, 0.0407, 0.0983, 0.0065,\n",
       "         0.0219, 0.0598, 0.0179, 0.0545, 0.0308, 0.0530, 0.0094, 0.0295, 0.0196,\n",
       "         0.0992, 0.0374, 0.0095, 0.0066, 0.0224, 0.0232, 0.0277, 0.0144, 0.0039],\n",
       "        [0.0327, 0.0254, 0.0200, 0.0225, 0.0168, 0.0097, 0.0222, 0.0701, 0.0086,\n",
       "         0.0217, 0.1622, 0.0049, 0.0158, 0.1530, 0.0111, 0.0244, 0.0018, 0.0284,\n",
       "         0.0101, 0.0044, 0.0052, 0.0132, 0.0431, 0.0080, 0.0376, 0.2173, 0.0095],\n",
       "        [0.0327, 0.0254, 0.0200, 0.0225, 0.0168, 0.0097, 0.0222, 0.0701, 0.0086,\n",
       "         0.0217, 0.1622, 0.0049, 0.0158, 0.1530, 0.0111, 0.0244, 0.0018, 0.0284,\n",
       "         0.0101, 0.0044, 0.0052, 0.0132, 0.0431, 0.0080, 0.0376, 0.2173, 0.0095],\n",
       "        [0.1064, 0.0334, 0.0071, 0.0495, 0.0130, 0.0927, 0.0034, 0.0335, 0.0318,\n",
       "         0.0148, 0.0245, 0.1443, 0.0234, 0.0112, 0.0143, 0.1465, 0.0115, 0.0536,\n",
       "         0.0247, 0.0184, 0.0149, 0.0022, 0.0293, 0.0303, 0.0032, 0.0402, 0.0217]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When we incentivise w to be near 0 the probabilities will all go to near 1 and thus is essentially the same as smoothing\n",
    "# Regularisation allows us to augment the loss function, we can take W**2 and sum those entries\n",
    "# because we are **2 there are no more signs, 0 loss if W is 0 any non 0 numbers loss is accummulated\n",
    "# This incentivises us to have weights close to 0 and thus forces a majority of them\n",
    "\n",
    "# Trying ot make all probabilities correct but also all Ws 0, we are minimising this regularisation loss\n",
    "# Adds a weight of W being close to 0 and thus this affects the amount of counts by making them non 0\n",
    "# the more we dominate using our  regularisation constant the more our weights are not allowed to grow\n",
    "# if we have a strong enough regularisation constant they will all be uniform predictions\n",
    "# Hence regularisation provides smoothing\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "index = 0\n",
    "for i in range(5):\n",
    "    # number betwen 0 and 1 that are sampled now we can normalise them and use torch.multinomial which will take the probaiility distribution, replacement means that when we draw an element we can put it back into them\n",
    "    # replacement is by default false within this generator\n",
    "    index = torch.multinomial(probs[index], num_samples = 1, replacement=True, generator=g).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
